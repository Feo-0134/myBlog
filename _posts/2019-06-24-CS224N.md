---
title: CS224n 自然语言理解 笔记
published: true
---

## [](#header-2) abstract

> Natural language processing (NLP) is one of the most important technologies of the information age, and a crucial part of artificial intelligence. Applications of NLP are everywhere because people communicate almost everything in language: web search, advertising, emails, customer service, language translation, medical reports, etc. In recent years, Deep Learning approaches have obtained very high performance across many different NLP tasks, using single end-to-end neural models that do not require traditional, task-specific feature engineering. In this course, a thorough introduction to cutting-edge research in Deep Learning for NLP is shown to the class. Through lectures, assignments and a final project, students will learn the necessary skills to design, implement, and understand their own neural network models.

## [](#header-2) Content

### [](#header-3) 梯度下降法(三种计算模式及其特点)
梯度下降法可用于优化线性回归模型

一般线性回归函数的假设函数形如: $h_θ = ∑^n_{j=0}θ_jx_j$

对应的损失函数为: $J_{train}(θ) = 1/(2m)∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})^2$

1. 批梯度下降  `Batch Gradient Descent` 每次更新使用了所有的训练数据, 是朝着最小值迭代运动的; 缺点是如果样本值很大的话，更新速度会很慢。

   迭代过程如下:

repeat{

$θ^{'}_j = θ_j + 1/m∑^m_{i=1}(y^i - h_θ(x^i))x^i_j​$

for( every j = 0, … n )

}

2. 随机梯度下降 `Stochastic Gradient Descent` 在每次更新的时候，只考虑了一个样本点，大大加快训练速度(批梯度下降的缺点); 缺点是有可能由于训练数据的噪声点较多，每一次利用噪声点进行更新的过程中不一定是朝着极小值方向更新

   迭代过程如下:

1> Randomly shuffle dataset:

2> 

repeat{

for i = 1, … m{

$θ^{'}_j =θ_j + (y^i - h_θ(x^i))x^i_j $

(for j = 0, … , n)

}	
}

3. 小批量梯度下降法 `Mini-Batch Gradient Descent` 是为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性 取二者优点综合而来，但是这里注意，不同问题的batch是不一样的。

   迭代过程如下:

Repeat{

for(i = 1, 11, 21, 31, … , 991) {

$θ^{'}_j = θ_j - α(1/10)∑^{i+9}_{k = i} (h_θ(x^k) - y^k)x^k_j$

(for every j = 0, … , n)

}

}

   

### [](#header-3) Logistic 回归 sigmoid 函数

logistic回归的抽象定义是对于二分类问题，能否直接预测出一个样本属于正样本的概率值. 具象化后便是如何将定义域 (-∞, +∞) 的值映射到 (0, 1) 且能完成分类任务.

设有超平面(可判别归类) $w^Tx + b$ 其中, w为权重向量, b 为偏置项, 是一个标量. 下式为将一个样本的特征向量映射成一个概率值 p(y=1|x) 的函数:

$h(x) = 1/(1 + exp(-w^Tx + b))$

这就是logistic回归的预测函数.

$log(p(y=1|x)/p(y=0|x)) > 0 $ 等价于  $w^Tx + b > 0$ 



最大似然估计求解

假设训练样本集为(xi，yi),*i*=1,...,*l*，其中xi是特征向量；yi为类别标签，取值为1或0。给定参数w和样本特征向量x，样本属于每个类的概率可以统一写成如下形式:

$p(y|x, w) = (h(x))^y(1-h(x))^{1-y}​$

通过最大似然估计来确定参数。由于样本之间相互独立，训练样本集的似然函数为：

$L(w) =𝛱^l_{i = 1}p(y_i|x_i,w) = 𝛱^l_{i = 1}(h(x_i)^{y_i}(1-h(x_i))^{1-y_i}) ​$

该函数对应n重伯努利分布, 对数似然函数为:

$f(w) = logL(w) = ∑^l_{i = 1}(y_ilogh(x_i)+(1- y_i)log(1 - h(x_i)))$

梯度下降法求解得权值迭代公式为:

$W_{k+1} = W_k - α∑^l_{i = 1}(h_w(x_i) - y_i)x_i$

### [](#header-3) 怎么求证高斯分布的均值和方差

### [](#header-3) 朴素贝叶斯建模及参数求解

### [](#header-3) 联合概率和边际概率之间的关系和计算
### [](#header-3) 区分和比较生成式模型与判别式模型(图模型


