---
title: CS224n è‡ªç„¶è¯­è¨€ç†è§£ ç¬”è®°
published: true
---

## [](#header-2) abstract

> Natural language processing (NLP) is one of the most important technologies of the information age, and a crucial part of artificial intelligence. Applications of NLP are everywhere because people communicate almost everything in language: web search, advertising, emails, customer service, language translation, medical reports, etc. In recent years, Deep Learning approaches have obtained very high performance across many different NLP tasks, using single end-to-end neural models that do not require traditional, task-specific feature engineering. In this course, a thorough introduction to cutting-edge research in Deep Learning for NLP is shown to the class. Through lectures, assignments and a final project, students will learn the necessary skills to design, implement, and understand their own neural network models.

## [](#header-2) Content

### [](#header-3) æ¢¯åº¦ä¸‹é™æ³•(ä¸‰ç§è®¡ç®—æ¨¡å¼åŠå…¶ç‰¹ç‚¹)
æ¢¯åº¦ä¸‹é™æ³•å¯ç”¨äºä¼˜åŒ–çº¿æ€§å›å½’æ¨¡å‹

ä¸€èˆ¬çº¿æ€§å›å½’å‡½æ•°çš„å‡è®¾å‡½æ•°å½¢å¦‚: $h_Î¸ = âˆ‘^n_{j=0}Î¸_jx_j$

å¯¹åº”çš„æŸå¤±å‡½æ•°ä¸º: $J_{train}(Î¸) = 1/(2m)âˆ‘^m_{i=1}(h_Î¸(x^{(i)})-y^{(i)})^2$

1. æ‰¹æ¢¯åº¦ä¸‹é™  `Batch Gradient Descent` æ¯æ¬¡æ›´æ–°ä½¿ç”¨äº†æ‰€æœ‰çš„è®­ç»ƒæ•°æ®, æ˜¯æœç€æœ€å°å€¼è¿­ä»£è¿åŠ¨çš„; ç¼ºç‚¹æ˜¯å¦‚æœæ ·æœ¬å€¼å¾ˆå¤§çš„è¯ï¼Œæ›´æ–°é€Ÿåº¦ä¼šå¾ˆæ…¢ã€‚

   è¿­ä»£è¿‡ç¨‹å¦‚ä¸‹:

repeat{

$Î¸^{'}_ j =Î¸_j + 1/mğ›´^m_{i=1}(y^i - h_Î¸(x^i))x^i_j$

for( every j = 0, â€¦ n )

}

2. éšæœºæ¢¯åº¦ä¸‹é™ `Stochastic Gradient Descent` åœ¨æ¯æ¬¡æ›´æ–°çš„æ—¶å€™ï¼Œåªè€ƒè™‘äº†ä¸€ä¸ªæ ·æœ¬ç‚¹ï¼Œå¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦(æ‰¹æ¢¯åº¦ä¸‹é™çš„ç¼ºç‚¹); ç¼ºç‚¹æ˜¯æœ‰å¯èƒ½ç”±äºè®­ç»ƒæ•°æ®çš„å™ªå£°ç‚¹è¾ƒå¤šï¼Œæ¯ä¸€æ¬¡åˆ©ç”¨å™ªå£°ç‚¹è¿›è¡Œæ›´æ–°çš„è¿‡ç¨‹ä¸­ä¸ä¸€å®šæ˜¯æœç€æå°å€¼æ–¹å‘æ›´æ–°

   è¿­ä»£è¿‡ç¨‹å¦‚ä¸‹:

1> Randomly shuffle dataset:

2> 

repeat{

for i = 1, â€¦ m{


$Î¸^{'}_ j =Î¸_j + (y^i - h_Î¸(x^i))x^i_j$


(for j = 0, â€¦ , n)

}	
}

3. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³• `Mini-Batch Gradient Descent` æ˜¯ä¸ºäº†è§£å†³æ‰¹æ¢¯åº¦ä¸‹é™æ³•çš„è®­ç»ƒé€Ÿåº¦æ…¢ï¼Œä»¥åŠéšæœºæ¢¯åº¦ä¸‹é™æ³•çš„å‡†ç¡®æ€§ å–äºŒè€…ä¼˜ç‚¹ç»¼åˆè€Œæ¥ï¼Œä½†æ˜¯è¿™é‡Œæ³¨æ„ï¼Œä¸åŒé—®é¢˜çš„batchæ˜¯ä¸ä¸€æ ·çš„ã€‚

   è¿­ä»£è¿‡ç¨‹å¦‚ä¸‹:

Repeat{

for(i = 1, 11, 21, 31, â€¦ , 991) 


$Î¸^{'}_ j =Î¸_j - Î±(1/10)ğ›´^{i+9}_ {k = i} (h_Î¸(x^k) - y^k)x^k_j$


(for every j = 0, â€¦ , n)



}

   

### [](#header-3) Logistic å›å½’

â€‹	logisticå‡½æ•°ï¼Œä¹Ÿå«sigmoidå‡½æ•°:  $y = 1/(1+e^{-x})$ 



â€‹	è‹¥ä»¤ $t = e^{-x}$         $y = 1/(1+t)$           $y^{'}_t = (0-1)/(1+t)^2$       $t^{'}_x = -e^{-x}$

â€‹	åˆ™æœ‰ $y^{'}_x = (e^{-x})/(1 + e^{-x})^2$ ä¸”å¯æ¨å¾— $y^{'} = y(1 - y)$


Logistic å›å½’ æ˜¯å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œç›´æ¥é¢„æµ‹å‡ºä¸€ä¸ªæ ·æœ¬å±äºæ­£æ ·æœ¬[0,1]èŒƒå›´å†…æ¦‚ç‡å€¼

logisticå›å½’ç”±Coxåœ¨1958å¹´æå‡º[1]ï¼Œè™½ç„¶å«å›å½’ï¼Œä½†è¿™æ˜¯ä¸€ç§äºŒåˆ†ç±»ç®—æ³•ï¼Œå¹¶ä¸”æ˜¯ä¸€ç§çº¿æ€§æ¨¡å‹ã€‚ç”±äºæ˜¯çº¿æ€§æ¨¡å‹ï¼Œå› æ­¤åœ¨é¢„æµ‹æ—¶è®¡ç®—ç®€å•ï¼Œåœ¨æŸäº›å¤§è§„æ¨¡åˆ†ç±»é—®é¢˜ï¼Œå¦‚å¹¿å‘Šç‚¹å‡»ç‡é¢„ä¼°ï¼ˆCTRï¼‰ä¸Šå¾—åˆ°äº†æˆåŠŸçš„åº”ç”¨ã€‚

æ ·æœ¬å±äºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬æ¦‚ç‡å€¼æ¯”çš„å¯¹æ•°ç§°ä¸ºå¯¹æ•°ä¼¼ç„¶æ¯”


$$
\log \frac{p(y=1 | x)}{p(y=0 | x)}=\log \frac{\frac{1}{1+\exp \left(-w^{T} x+b\right)}}{1-\frac{1}{1+\exp \left(-w^{T} x+b\right)}}=w^{T} x+b
$$


#### æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ±‚è§£

â€‹	ç”±äºæ ·æœ¬ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œè®­ç»ƒæ ·æœ¬é›†çš„ä¼¼ç„¶å‡½æ•°ä¸ºï¼š


$$
L(\mathrm{w})=\prod_{i=1}^{l} p\left(y_{i} | \mathrm{x}_{i}, \mathrm{w}\right)=\prod_{i=1}^{l}\left(h\left(\mathrm{x}_{i}\right)^{y_{i}}\left(1-h\left(\mathrm{x}_{i}\right)\right)^{1-y_{i}}\right)
$$


â€‹	è¿™ä¸ªå‡½æ•°å¯¹åº”äºné‡ä¼¯åŠªåˆ©åˆ†å¸ƒã€‚å¯¹æ•°ä¼¼ç„¶å‡½æ•°ä¸º:


$$
f(\mathrm{w})=\log L(\mathrm{w})=\sum_{i=1}^{l}\left(y_{i} \log h\left(\mathrm{x}_{i}\right)+\left(1-y_{i}\right) \log \left(1-h\left(\mathrm{x}_{i}\right)\right)\right)
$$


ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ±‚è§£


$$
-\nabla \sum_{i=1}^{l}\left(y_{i} \log h\left(\mathbf{x}_{i}\right)+\left(1-y_{i}\right) \log \left(1-h\left(\mathrm{x}_{i}\right)\right)\right)
$$

$$
=-\sum_{k=1}^{l}\left(y_{i} \frac{1}{h\left(\mathbf{x}_{i}\right)} h\left(\mathbf{x}_{i}\right)\left(1-h\left(\mathbf{x}_{i}\right)\right) \mathbf{x}_{i}+\left(1-y_{i}\right) \frac{1}{1-h\left(\mathbf{x}_{i}\right)}(-1) h\left(\mathbf{x}_{i}\right)\left(1-h\left(\mathbf{x}_{i}\right)\right) \mathbf{x}_{i}\right)
$$

$$
=-\sum_{i=1}^{l}\left(y_{i}\left(1-h\left(\mathbf{x}_{i}\right)\right) \mathbf{x}_{i}-\left(1-y_{i}\right) h\left(\mathrm{x}_{i}\right) \mathrm{x}_{i}\right)
$$

$$
=\sum_{i=1}^{l}\left(h\left(\mathbf{x}_{i}\right)-y_{i}\right) \mathbf{x}_{i}
$$



æœ€åå¾—åˆ°æƒé‡çš„æ¢¯åº¦ä¸‹é™æ³•çš„è¿­ä»£æ›´æ–°å…¬å¼ä¸ºï¼š
$$
\mathbf{w}_{k+1}=\mathbf{w}_{k}-\alpha \sum_{i=1}^{l}\left(h_{\mathrm{w}}\left(\mathrm{x}_{i}\right)-y_{i}\right) \mathrm{x}_{i}
$$

### [](#header-3) æ€ä¹ˆæ±‚è¯é«˜æ–¯åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®
### [](#header-3) æœ´ç´ è´å¶æ–¯å»ºæ¨¡åŠå‚æ•°æ±‚è§£

### [](#header-3) è”åˆæ¦‚ç‡å’Œè¾¹é™…æ¦‚ç‡ä¹‹é—´çš„å…³ç³»å’Œè®¡ç®—
### [](#header-3) åŒºåˆ†å’Œæ¯”è¾ƒç”Ÿæˆå¼æ¨¡å‹ä¸åˆ¤åˆ«å¼æ¨¡å‹(å›¾æ¨¡å‹


