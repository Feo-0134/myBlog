---
title: CS224n 自然语言理解 笔记
published: true
---

## [](#header-2) abstract

> Natural language processing (NLP) is one of the most important technologies of the information age, and a crucial part of artificial intelligence. Applications of NLP are everywhere because people communicate almost everything in language: web search, advertising, emails, customer service, language translation, medical reports, etc. In recent years, Deep Learning approaches have obtained very high performance across many different NLP tasks, using single end-to-end neural models that do not require traditional, task-specific feature engineering. In this course, a thorough introduction to cutting-edge research in Deep Learning for NLP is shown to the class. Through lectures, assignments and a final project, students will learn the necessary skills to design, implement, and understand their own neural network models.

## [](#header-2) Content

### [](#header-3) 梯度下降法(三种计算模式及其特点)
梯度下降法可用于优化线性回归模型

一般线性回归函数的假设函数形如: $h_θ = ∑^n_{j=0}θ_jx_j$

对应的损失函数为: $J_{train}(θ) = 1/(2m)∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})^2$

1. 批梯度下降  `Batch Gradient Descent` 每次更新使用了所有的训练数据, 是朝着最小值迭代运动的; 缺点是如果样本值很大的话，更新速度会很慢。

   迭代过程如下:

repeat{

$θ^{'}_ j =θ_j + 1/m𝛴^m_{i=1}(y^i - h_θ(x^i))x^i_j$

for( every j = 0, … n )

}

2. 随机梯度下降 `Stochastic Gradient Descent` 在每次更新的时候，只考虑了一个样本点，大大加快训练速度(批梯度下降的缺点); 缺点是有可能由于训练数据的噪声点较多，每一次利用噪声点进行更新的过程中不一定是朝着极小值方向更新

   迭代过程如下:

1> Randomly shuffle dataset:

2> 

repeat{

for i = 1, … m{


$θ^{'}_ j =θ_j + (y^i - h_θ(x^i))x^i_j$


(for j = 0, … , n)

}	
}

3. 小批量梯度下降法 `Mini-Batch Gradient Descent` 是为了解决批梯度下降法的训练速度慢，以及随机梯度下降法的准确性 取二者优点综合而来，但是这里注意，不同问题的batch是不一样的。

   迭代过程如下:

Repeat{

for(i = 1, 11, 21, 31, … , 991) 


$θ^{'}_ j =θ_j - α(1/10)𝛴^{i+9}_ {k = i} (h_θ(x^k) - y^k)x^k_j$


(for every j = 0, … , n)



}

   

### [](#header-3) Logistic 回归

​	logistic函数，也叫sigmoid函数:  $y = 1/(1+e^{-x})$ 



​	若令 $t = e^{-x}$         $y = 1/(1+t)$           $y^{'}_t = (0-1)/(1+t)^2$       $t^{'}_x = -e^{-x}$

​	则有 $y^{'}_x = (e^{-x})/(1 + e^{-x})^2$ 且可推得 $y^{'} = y(1 - y)$


Logistic 回归 是对于二分类问题，直接预测出一个样本属于正样本[0,1]范围内概率值

logistic回归由Cox在1958年提出[1]，虽然叫回归，但这是一种二分类算法，并且是一种线性模型。由于是线性模型，因此在预测时计算简单，在某些大规模分类问题，如广告点击率预估（CTR）上得到了成功的应用。

样本属于正样本和负样本概率值比的对数称为对数似然比


$$
\log \frac{p(y=1 | x)}{p(y=0 | x)}=\log \frac{\frac{1}{1+\exp \left(-w^{T} x+b\right)}}{1-\frac{1}{1+\exp \left(-w^{T} x+b\right)}}=w^{T} x+b
$$


#### 最大似然估计求解

​	由于样本之间相互独立，训练样本集的似然函数为：


$$
L(\mathrm{w})=\prod_{i=1}^{l} p\left(y_{i} | \mathrm{x}_{i}, \mathrm{w}\right)=\prod_{i=1}^{l}\left(h\left(\mathrm{x}_{i}\right)^{y_{i}}\left(1-h\left(\mathrm{x}_{i}\right)\right)^{1-y_{i}}\right)
$$


​	这个函数对应于n重伯努利分布。对数似然函数为:


$$
f(\mathrm{w})=\log L(\mathrm{w})=\sum_{i=1}^{l}\left(y_{i} \log h\left(\mathrm{x}_{i}\right)+\left(1-y_{i}\right) \log \left(1-h\left(\mathrm{x}_{i}\right)\right)\right)
$$


使用梯度下降求解


$$
-\nabla \sum_{i=1}^{l}\left(y_{i} \log h\left(\mathbf{x}_{i}\right)+\left(1-y_{i}\right) \log \left(1-h\left(\mathrm{x}_{i}\right)\right)\right)
$$

$$
=-\sum_{k=1}^{l}\left(y_{i} \frac{1}{h\left(\mathbf{x}_{i}\right)} h\left(\mathbf{x}_{i}\right)\left(1-h\left(\mathbf{x}_{i}\right)\right) \mathbf{x}_{i}+\left(1-y_{i}\right) \frac{1}{1-h\left(\mathbf{x}_{i}\right)}(-1) h\left(\mathbf{x}_{i}\right)\left(1-h\left(\mathbf{x}_{i}\right)\right) \mathbf{x}_{i}\right)
$$

$$
=-\sum_{i=1}^{l}\left(y_{i}\left(1-h\left(\mathbf{x}_{i}\right)\right) \mathbf{x}_{i}-\left(1-y_{i}\right) h\left(\mathrm{x}_{i}\right) \mathrm{x}_{i}\right)
$$

$$
=\sum_{i=1}^{l}\left(h\left(\mathbf{x}_{i}\right)-y_{i}\right) \mathbf{x}_{i}
$$



最后得到权重的梯度下降法的迭代更新公式为：
$$
\mathbf{w}_{k+1}=\mathbf{w}_{k}-\alpha \sum_{i=1}^{l}\left(h_{\mathrm{w}}\left(\mathrm{x}_{i}\right)-y_{i}\right) \mathrm{x}_{i}
$$

### [](#header-3) 怎么求证高斯分布的均值和方差
### [](#header-3) 朴素贝叶斯建模及参数求解

### [](#header-3) 联合概率和边际概率之间的关系和计算
### [](#header-3) 区分和比较生成式模型与判别式模型(图模型


